The model:
TinyModel(
  (linear1): Linear(in_features=100, out_features=200, bias=True)
  (activation): ReLU()
  (linear2): Linear(in_features=200, out_features=10, bias=True)
  (softmax): Softmax(dim=None)
)


Just one layer:
Linear(in_features=200, out_features=10, bias=True)


Model params:
Parameter containing:
tensor([[-0.0611, -0.0060,  0.0921,  ..., -0.0936, -0.0473, -0.0284],
        [-0.0313, -0.0660, -0.0588,  ..., -0.0273, -0.0160,  0.0702],
        [-0.0576, -0.0256,  0.0957,  ..., -0.0968, -0.0437,  0.0760],
        ...,
        [ 0.0705,  0.0026,  0.0576,  ..., -0.0932, -0.0184, -0.0691],
        [-0.0179, -0.0101,  0.0163,  ..., -0.0321,  0.0762, -0.0362],
        [ 0.0255,  0.0505,  0.0188,  ..., -0.0701, -0.0481, -0.0677]],
       requires_grad=True)
Parameter containing:
tensor([ 0.0027, -0.0254, -0.0178, -0.0680, -0.0876,  0.0851,  0.0829,  0.0110,
        -0.0428,  0.0823, -0.0452, -0.0034, -0.0966, -0.0858,  0.0558,  0.0936,
        -0.0621, -0.0602,  0.0511,  0.0055,  0.0277,  0.0639,  0.0355, -0.0930,
        -0.0862,  0.0024,  0.0951, -0.0831, -0.0892,  0.0975,  0.0013,  0.0143,
         0.0159,  0.0066, -0.0669, -0.0161, -0.0968, -0.0519,  0.0688, -0.0469,
         0.0825,  0.0503, -0.0225,  0.0592,  0.0968,  0.0038, -0.0304,  0.0770,
        -0.0045, -0.0537, -0.0190,  0.0015, -0.0622,  0.0723, -0.0917,  0.0963,
        -0.0288, -0.0593, -0.0184, -0.0191,  0.0391, -0.0195,  0.0857, -0.0961,
         0.0413,  0.0257, -0.0758, -0.0008,  0.0071,  0.0715,  0.0368,  0.0239,
        -0.0212, -0.0216,  0.0575, -0.0110, -0.0545,  0.0999, -0.0341, -0.0097,
        -0.0001, -0.0610, -0.0882,  0.0697, -0.0666, -0.0333, -0.0728, -0.0118,
         0.0720,  0.0270,  0.0501,  0.0627,  0.0542,  0.0420, -0.0805, -0.0258,
         0.0972,  0.0066,  0.0441, -0.0832,  0.0608, -0.0779,  0.0919,  0.0023,
         0.0013,  0.0427,  0.0638, -0.0327,  0.0364, -0.0437, -0.0570,  0.0978,
        -0.0821,  0.0140, -0.0420,  0.0222, -0.0520,  0.0464,  0.0046,  0.0516,
         0.0220, -0.0886,  0.0551,  0.0588,  0.0712, -0.0451, -0.0055,  0.0711,
        -0.0175,  0.0807,  0.0114, -0.0973,  0.0386,  0.0544, -0.0278,  0.0472,
         0.0336,  0.0569,  0.0283,  0.0898,  0.0157,  0.0989,  0.0375,  0.0315,
        -0.0274, -0.0530, -0.0359,  0.0681,  0.0076,  0.0630,  0.0111,  0.0872,
        -0.0391,  0.0705, -0.0637, -0.0202, -0.0131,  0.0557, -0.0543,  0.0537,
        -0.0888,  0.0841,  0.0262,  0.0819,  0.0301, -0.0252,  0.0925,  0.0340,
        -0.0338, -0.0392,  0.0994, -0.0801,  0.0006,  0.0842,  0.0445,  0.0408,
         0.0894, -0.0994,  0.0049,  0.0690, -0.0840, -0.0547, -0.0760, -0.0147,
         0.0510,  0.0549, -0.0751, -0.0694,  0.0436, -0.0251, -0.0685, -0.0376,
         0.0156,  0.0102,  0.0143, -0.0643, -0.0378, -0.0124,  0.0941, -0.0043],
       requires_grad=True)
Parameter containing:
tensor([[-0.0595,  0.0289, -0.0104,  ...,  0.0158,  0.0216, -0.0637],
        [ 0.0408, -0.0002,  0.0318,  ...,  0.0101,  0.0587, -0.0356],
        [-0.0265,  0.0176,  0.0061,  ..., -0.0655,  0.0018, -0.0323],
        ...,
        [ 0.0018, -0.0051, -0.0014,  ...,  0.0320,  0.0272, -0.0682],
        [ 0.0386, -0.0112,  0.0186,  ..., -0.0434, -0.0419,  0.0340],
        [ 0.0293, -0.0063, -0.0213,  ..., -0.0101,  0.0684,  0.0696]],
       requires_grad=True)
Parameter containing:
tensor([ 0.0022, -0.0526,  0.0420, -0.0657,  0.0108,  0.0127,  0.0403,  0.0294,
         0.0384, -0.0701], requires_grad=True)


Layer params:
Parameter containing:
tensor([[-0.0595,  0.0289, -0.0104,  ...,  0.0158,  0.0216, -0.0637],
        [ 0.0408, -0.0002,  0.0318,  ...,  0.0101,  0.0587, -0.0356],
        [-0.0265,  0.0176,  0.0061,  ..., -0.0655,  0.0018, -0.0323],
        ...,
        [ 0.0018, -0.0051, -0.0014,  ...,  0.0320,  0.0272, -0.0682],
        [ 0.0386, -0.0112,  0.0186,  ..., -0.0434, -0.0419,  0.0340],
        [ 0.0293, -0.0063, -0.0213,  ..., -0.0101,  0.0684,  0.0696]],
       requires_grad=True)
Parameter containing:
tensor([ 0.0022, -0.0526,  0.0420, -0.0657,  0.0108,  0.0127,  0.0403,  0.0294,
         0.0384, -0.0701], requires_grad=True)


# Common Layer Types - Linear Layers
Input:
tensor([[0.5519, 0.5866, 0.5356]])


Weight and Bias parameters:
Parameter containing:
tensor([[-0.1982,  0.4837, -0.2419],
        [-0.5117,  0.4004, -0.1725]], requires_grad=True)
Parameter containing:
tensor([-0.3837, -0.5703], requires_grad=True)


Output:
tensor([[-0.3388, -0.7103]], grad_fn=<AddmmBackward0>)


# Common Layer Types - Convolutional Layers (LeNet5)
The LeNet5:
LeNet(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
  (fc1): Linear(in_features=576, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)


# Common Layer Types - Recurrent Layers (RNN)
The LSTM:
LSTMTagger(
  (word_embeddings): Embedding(100, 10)
  (lstm): LSTM(10, 10)
  (hidden2tag): Linear(in_features=10, out_features=1, bias=True)
)


# Common Layer Types - Transformer
TransformerModel(
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)
        )
        (linear1): Linear(in_features=200, out_features=200, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (linear2): Linear(in_features=200, out_features=200, bias=True)
        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (dropout2): Dropout(p=0.2, inplace=False)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)
        )
        (linear1): Linear(in_features=200, out_features=200, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (linear2): Linear(in_features=200, out_features=200, bias=True)
        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (dropout2): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (encoder): Embedding(1000, 200)
  (decoder): Linear(in_features=200, out_features=1000, bias=True)
)
