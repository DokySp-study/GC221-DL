
# Data Manipulation Layers - MaxPool
tensor([[[0.0868, 0.0894, 0.5970, 0.6731, 0.9672, 0.3609],
         [0.3164, 0.6909, 0.0384, 0.9525, 0.2017, 0.4562],
         [0.3889, 0.9045, 0.9972, 0.0840, 0.5583, 0.1598],
         [0.4591, 0.2377, 0.5200, 0.7759, 0.5925, 0.3961],
         [0.5717, 0.5593, 0.4027, 0.2487, 0.7010, 0.3781],
         [0.7462, 0.2632, 0.3202, 0.8810, 0.5030, 0.1458]]])
tensor([[[0.9972, 0.9672],
         [0.7462, 0.8810]]])

# Data Manipulation Layers - Batch Normalization
tensor([[[ 5.2965, 13.8388, 10.3267, 13.8339],
         [12.8015, 13.9710, 11.7400, 24.2731],
         [20.3688, 22.4265,  5.8260, 14.5590],
         [23.7773, 19.4998,  6.8579, 22.8056]]])
tensor(15.1376)
tensor([[[-1.5801,  0.8618, -0.1422,  0.8604],
         [-0.5773, -0.3441, -0.7890,  1.7105],
         [ 0.7104,  1.0300, -1.5484, -0.1920],
         [ 0.8202,  0.1872, -1.6837,  0.6764]]],
       grad_fn=<NativeBatchNormBackward0>)
tensor(1.4901e-08, grad_fn=<MeanBackward0>)

# Data Manipulation Layers - Dropout
tensor([[[0.0000, 1.3288, 0.0000, 0.0000],
         [0.7746, 0.0000, 0.0000, 0.0000],
         [0.0000, 1.4805, 0.1503, 0.2742],
         [0.0000, 0.0000, 1.3895, 0.0000]]])
tensor([[[0.2015, 0.0000, 0.5510, 0.0000],
         [0.7746, 0.0000, 1.5520, 0.5390],
         [0.7774, 1.4805, 0.1503, 0.0000],
         [0.9587, 0.0000, 0.0000, 1.3256]]])

# Data Manipulation Layers - Activation Functions
tensor(10.)
tensor(0.)
tensor(-0.1000)

# Data Manipulation Layers - Loss Functions
tensor(0.7067)
tensor(1.0444)
